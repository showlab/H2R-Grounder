# **H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos**

**Hai Ci, Xiaokang Liu, Pei Yang, Yiren Song, Mike Zheng Shou***
**Show Lab, National University of Singapore**
*Corresponding author

ğŸ“„ **Paper (arXiv):** *coming soon*
ğŸŒ **Project Page:** [https://showlab.github.io/H2R-Grounder/](https://showlab.github.io/H2R-Grounder/)

---

## âš¡ TL;DR

**H2R-Grounder converts third-person human interaction videos into frame-aligned robot manipulation videos â€” using *no paired humanâ€“robot data* for training.**

---

## ğŸ“· Method Overview

<p align="center">
  <img src="docs/pipeline.png" width="85%" alt="H2R-Grounder Pipeline"/>
</p>

**Figure:** *H2R-Grounder pipeline.*
We extract pose and background to form H2Rep, then use a diffusion-based in-context model to generate physically grounded robot videos aligned with human actions.

---

## ğŸ¥ Qualitative Results

Visit our project page for **full videos**, comparisons, ablations, and failure case analysis:

ğŸ‘‰ [https://showlab.github.io/H2R-Grounder/](https://showlab.github.io/H2R-Grounder/)

---

## ğŸ“¦ Code & Models

Code and models will be released soon.

---

## âœï¸ Citation

```bibtex
@article{ci2025h2rgrounder,
  title={H2R-Grounder: Paired-Data-Free Translation of Human Interaction Videos into Physically Grounded Robot Videos},
  author={Ci, Hai and Liu, Xiaokang and Yang, Pei and Song, Yiren and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:XXXXX},
  year={2025}
}
```
